{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "# from models.model import rnn_model\n",
    "# from dataset.poems import process_poems,generate_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_integer(\"batch_size\",64,\"batch_siz=?\")\n",
    "tf.app.flags.DEFINE_float(\"learing_rate\",0.01,\"learing_rate\")\n",
    "tf.app.flags.DEFINE_string(\"check_pointss_dir\",\"./model/\",\"check_pointss_dir\")\n",
    "tf.app.flags.DEFINE_string(\"file_path\",\"./data/.txt\",\"file_path\")\n",
    "tf.app.flags.DEFINE_integer(\"epoch\",50,\"train_epoch\")\n",
    "\n",
    "start_token=\"G\"\n",
    "end_token=\"E\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简单的数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_poems(file_name):\n",
    "    poems=[]\n",
    "    with open(file_name,\"r\",encoding=\"UTF-8\") as f:\n",
    "        for  line in f.readlines():\n",
    "            title,content=line.strip().split(\":\")\n",
    "            if \"_\" in content or \"{\" in content:\n",
    "                continue\n",
    "            if len(content)<5 or len(content)>80:\n",
    "                continue\n",
    "            content=start_token+content+end_token\n",
    "            poems.append(content)\n",
    "            \n",
    "    poems=sorted(poems,key=lambda l:len(line))     \n",
    "    all_words=[]\n",
    "    for poem  in poems:            #对每一句进 行处理            \n",
    "        all_words+=[word for word in poem]         #每一句中的每个词\n",
    "    counter =collections.Counter(all_words)      #以字典的新式显示每个字出现的次数\n",
    "    count_paris=sorted(Counter.items(),key=lambda x:x[-1])\n",
    "    words,cou=zip(*count_paris)        #zip函数进行关联，前面加*号代表反向处理，即从元祖列表转换成分离的列表返回\n",
    "    words=words[:len(words)]\n",
    "    words_int_map=dict(zip(words,range(len(words))))       #映射词对应数字\n",
    "    poems_vector=[list(map(lambda word:words_int_map.get(word,len(words))))]\n",
    "    \n",
    "    return poems_vector,words_int_map,words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch数据制作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-20deb7da9f53>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-20deb7da9f53>\"\u001b[1;36m, line \u001b[1;32m14\u001b[0m\n\u001b[1;33m    y_data[:,:-1]=x_data[:,1;]\u001b[0m\n\u001b[1;37m                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def generate_batch(batch_size,poems_vec,word_to_int):\n",
    "    n_chunk=len(poems_vec)/batch_size\n",
    "    x_batches=[]\n",
    "    y_batches=[]\n",
    "    for i in range(n_chunk):\n",
    "        start_index=i*batch_size\n",
    "        end_index=start_index+batch_size\n",
    "        batches=poems_vec[start_index:end_index]\n",
    "        length=max(map(len,batches))\n",
    "        x_data=np.full((batch_size,length),word_to_int(\" \"),np.int32)\n",
    "        for row in range(batch_size):\n",
    "            x_data[row,:len(batches[row])]=batches[row]\n",
    "        y_data=np.copy(x_data)\n",
    "        y_data[:,:-1]=x_data[:,1;]\n",
    "        x_batches.append(x_data)\n",
    "        y_batches.append(y_data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (<ipython-input-8-66759096146c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-66759096146c>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    def rnn_model(model=\"lstm\",input_data,output_data,vocab_size,rnn_size=128,num_layers=2,batch_size=64,learing_rate=0.001):\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "def rnn_model(model=\"lstm\",input_data,output_data,vocab_size,rnn_size=128,num_layers=2,batch_size=64,learing_rate=0.001):\n",
    "    end_points={}\n",
    "    lstm_cell=tf.nn.rnn_cell.BasicLSTMCell(n_hidden_units,forget_bias=1.0,state_is_tuple=True)\n",
    "    cell=tf.contrib.rnn.MultiRNNCell([cell]*num_layers,state_is_tuple=True)\n",
    "    cell=lstm_cell.zero_state(batch_size,dtype=tf.float32)\n",
    "    embedding=tf.Variable(tf.random_uniform([vocab_size+1,rnn_size],-1.0,1.0))   #区间在-1到1之间\n",
    "    inputs=tf.nn.embedding_lookup(embedding,input_data)     #选取一个张量里面索引对应的元素\n",
    "\n",
    "    outputs,states=tf.nn.dynamic_rnn(cell,inputs,initial_state=_initial_state) \n",
    "    #用于创建由RNNCell细胞指定是循环神经网络，对inputs进行循环展开。\n",
    "    \n",
    "    output=tf,reshape(outputs,[-1,rnn_size])\n",
    "    \n",
    "    \n",
    "    weights=tf.Variable(tf.random_normal([rnn_size,vocab_size+1]))\n",
    "    biase=tf.Variable(tf.zeros(shape=[vocab_size+1]))\n",
    "    logits=tf.nn.biase_add(tf.matmul(output,weights),biase)\n",
    "    \n",
    "    \n",
    "    if output_data is not None:\n",
    "        labels=tf.one_hot(tf.reshape(output_data,[-1]),depth=vocab_size+1)\n",
    "        loss=tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=logits)\n",
    "        total_loss=tf.reduce_mean(loss)\n",
    "        train=tf.train.Adamoptimizer(learing_rate).mimimize(total_loss)\n",
    "        \n",
    "        \n",
    "        end_points[\"init\"]=initial_state\n",
    "        end_points[\"output\"]=output\n",
    "        end_points[\"train\"]=train\n",
    "        end_points[\"total_loss\"]=total_loss\n",
    "        end_points[\"loss\"]=loss\n",
    "        end_points[\"last_state\"]=last_state\n",
    "        \n",
    "    else:\n",
    "        pred=tf.nn.softmax(logits)\n",
    "        \n",
    "        end_points[\"init\"]=initial_state\n",
    "        end_points[\"last_state\"]=last_state\n",
    "        end_points[\"pred\"]=pred\n",
    "        \n",
    "    return end_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    poems_vector,word_to_int,vocabularies=process_poems(FLAGS.file_path)\n",
    "    batch_inputs,batch_outputs=generate_batch(FLAGS.batch_size,poems_vector,word_to_int)\n",
    "    \n",
    "def main(is_train):\n",
    "    if is_train:\n",
    "        print(\"training\")\n",
    "        run_training()\n",
    "    else:\n",
    "        print(\"test\")\n",
    "        begin_word=input(\"word:\")\n",
    "        \n",
    "if _name_==\"_main_\":\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
